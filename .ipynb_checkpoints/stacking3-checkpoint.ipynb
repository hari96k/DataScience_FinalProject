{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started feature processing....\n",
      "Finished feature processing....\n",
      "Started learning....\n",
      "Analyzing classifier: 1\n",
      "Analyzing fold 0\n",
      "Analyzing fold 1\n",
      "Analyzing fold 2\n",
      "Analyzing fold 3\n",
      "Analyzing classifier: 2\n",
      "Analyzing fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing fold 1\n",
      "Analyzing fold 2\n",
      "Analyzing fold 3\n",
      "Analyzing classifier: 3\n",
      "Analyzing fold 0\n",
      "Analyzing fold 1\n",
      "Analyzing fold 2\n",
      "Analyzing fold 3\n",
      "Analyzing classifier: 4\n",
      "Analyzing fold 0\n",
      "Analyzing fold 1\n",
      "Analyzing fold 2\n",
      "Analyzing fold 3\n",
      "Stacking has started....\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, LassoCV, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pred(clf, test):\n",
    "\n",
    "\tif hasattr(clf, 'predict_proba'):\n",
    "\t\treturn clf.predict_proba(test)[:, 1]\n",
    "\treturn clf.predict(test)\n",
    "\n",
    "selected_features = ['TXILALEV',\n",
    " 'COCEVER',\n",
    " 'MMLS5LBS',\n",
    " 'CIGEVER',\n",
    " 'IICGINCR',\n",
    " 'IICGNCG',\n",
    " 'ANLFFLSP',\n",
    " 'MMBTREC2',\n",
    " 'HL30EST',\n",
    " 'HALEMCTD',\n",
    " 'DNICNSP',\n",
    " 'FUCIG21',\n",
    " 'BLNTYFU',\n",
    " 'MRJFMCTD',\n",
    " 'CRKEVER',\n",
    " 'MMTRD30D',\n",
    " 'IICD2YFU',\n",
    " 'TXNDILAL',\n",
    " 'IICGAVD',\n",
    " 'MJEVER',\n",
    " 'PROBATON',\n",
    " 'CIGFLAG',\n",
    " 'FUMJ21',\n",
    " 'MRJPHCTD',\n",
    " 'CPNPSYFG',\n",
    " 'MEDMJALL',\n",
    " 'IICGRGNM',\n",
    " 'FUSUM21',\n",
    " 'HALKPLMT',\n",
    " 'MRJYRBFR',\n",
    " 'TOBMON',\n",
    " 'IEMFLAG',\n",
    " 'HALLREC',\n",
    " 'IICGSLHR',\n",
    " 'MRJFLAG',\n",
    " 'CR30EST',\n",
    " 'STIMNEWC',\n",
    " 'IIMJYFU',\n",
    " 'IICGCRGP',\n",
    " 'COCNEEDL',\n",
    " 'COCKPLMT',\n",
    " 'SLTYFU',\n",
    " 'MMTLGMS',\n",
    " 'STMFFOSP',\n",
    " 'FUCD221',\n",
    " 'IICOCAGE',\n",
    " 'COCPHCTD',\n",
    " 'HALFLAG']\n",
    "\n",
    "selected_features.append('BOOKED')\n",
    "\n",
    "print (\"Started feature processing....\")\n",
    "#feature processing\n",
    "dataset = pd.read_csv(\"raw.csv\")\n",
    "\n",
    "dataset = dataset[selected_features]\n",
    "\n",
    "#change all response variables to binary\n",
    "dataset['BOOKED'] = dataset['BOOKED'].map({1: 1, 2: 0, 3: 1, 85: 0, 94: 0, 97: 0, 98: 0})\n",
    "\n",
    "dataset.to_csv(\"raw_mod.csv\", index=False)\n",
    "\n",
    "training_data, testing_data = train_test_split(dataset, train_size=0.5)\n",
    "\n",
    "training_data.reset_index(drop=True, inplace=True)\n",
    "testing_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_result = np.asarray(testing_data.BOOKED) #will test on these y values\n",
    "y = np.asarray(training_data.BOOKED)\n",
    "\n",
    "training_features = training_data.copy()\n",
    "testing_features = testing_data.copy()\n",
    "\n",
    "del training_features['BOOKED']\n",
    "del testing_features['BOOKED']\n",
    "\n",
    "complete_feature_matrix = pd.concat([training_features, testing_features])\n",
    "\n",
    "transformed_data = complete_feature_matrix\n",
    "\n",
    "transformed_training_data = np.asarray(transformed_data[:training_data.shape[0]])\n",
    "transformed_testing_data = np.asarray(transformed_data[training_data.shape[0]:])\n",
    "\n",
    "print (\"Finished feature processing....\")\n",
    "\n",
    "\n",
    "# base_clfs = [xgb.XGBRegressor(n_estimators=590, max_depth=4, learning_rate=0.12, objective=\"binary:logistic\"),\n",
    "# \t\t\txgb.XGBRegressor(n_estimators=514, max_depth=3, learning_rate=0.01925, objective=\"binary:logistic\"),\n",
    "# \t\t\txgb.XGBRegressor(n_estimators=350, max_depth=5, learning_rate=0.03, objective=\"binary:logistic\"),\n",
    "# \t\t\tRandomForestClassifier(n_estimators=450, max_depth=6, n_jobs=-1, criterion='gini', max_features='auto')]\n",
    "\n",
    "base_clfs = [RandomForestClassifier(n_estimators=450, max_depth=6, n_jobs=-1, criterion='gini', max_features='auto'), \n",
    "\t\t\t  LogisticRegression(C=10, solver='sag'), LinearRegression(), xgb.XGBRegressor(n_estimators=350, max_depth=5, learning_rate=0.03, objective=\"binary:logistic\")]\n",
    "\n",
    "print (\"Started learning....\")\n",
    "\n",
    "\n",
    "#ExtraTreesClassifier(n_estimators=300, n_jobs=-1, criterion='entropy')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "folds = list()\n",
    "\n",
    "for (train, test) in skf.split(transformed_training_data, y):\n",
    "\tfolds.append((train, test))\n",
    "\n",
    "stack_train_data = np.zeros((training_data.shape[0], len(base_clfs)))\n",
    "stack_test_data = np.zeros((testing_data.shape[0], len(base_clfs)))\n",
    "\n",
    "\n",
    "for i, clf in enumerate(base_clfs):\n",
    "\tprint (\"Analyzing classifier: \" + str(i + 1))\n",
    "\tstack_test_data_i = np.zeros((testing_data.shape[0], len(folds)))\n",
    "\tfor j, (train, test) in enumerate(folds):\n",
    "\t\tprint (\"Analyzing fold \" +  str(j))\n",
    "\t\tX_train, X_test = transformed_training_data[train], transformed_training_data[test]\n",
    "\t\ty_train, y_test = y[train], y[test]\n",
    "\t\tclf.fit(X_train, y_train)\n",
    "\t\ty_pred = pred(clf, X_test)\n",
    "\t\tstack_train_data[test, i] = y_pred\n",
    "\t\tstack_test_data_i[:, j] = pred(clf, transformed_testing_data)\n",
    "\tstack_test_data[:, i] = stack_test_data_i.mean(1)\n",
    "\n",
    "print (\"Stacking has started....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result normalization has started....\n",
      "AUC: 0.867227436015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meta_clf = xgb.XGBRegressor(n_estimators=514, max_depth=3, learning_rate=0.01925, objective=\"binary:logistic\")\n",
    "meta_clf.fit(stack_train_data, y)\n",
    "y_pred = meta_clf.predict(stack_test_data)\n",
    "y_pred_train = meta_clf.predict(stack_train_data)\n",
    "\n",
    "# y_pred = meta_clf.predict_proba(stack_test_data)[:, 1]\n",
    "# y_pred_train = meta_clf.predict_proba(stack_train_data)[:, 1]\n",
    "\n",
    "print (\"Result normalization has started....\")\n",
    "\n",
    "y_pred = (y_pred - y_pred.min())/(y_pred.max() - y_pred.min())\n",
    "\n",
    "y_pred_train = (y_pred_train - y_pred_train.min())/(y_pred_train.max() - y_pred_train.min())\n",
    "\n",
    "print (\"AUC: \" + str(metrics.roc_auc_score(y_result, y_pred)))\n",
    "\n",
    "# meta_clf = LogisticRegression()\n",
    "# meta_clf.fit(transformed_training_data, y)\n",
    "# y_pred = meta_clf.predict_proba(transformed_testing_data)[:, 1]\n",
    "# y_pred = (y_pred - y_pred.min())/(y_pred.max() - y_pred.min())\n",
    "# print \"AUC: \" + str(metrics.roc_auc_score(y_result, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
